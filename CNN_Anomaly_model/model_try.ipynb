{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git add ./\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--d\", default = \"Glove\", type=str)\n",
    "parser.add_argument(\"--t\", default = True, type=bool)\n",
    "parser.add_argument(\"--maxLen\", default =20, type=int)\n",
    "parser.add_argument(\"--BatchSize\", default =3, type=int)\n",
    "parser.add_argument(\"--embedLen\", default = 300, type=int)\n",
    "parser.add_argument(\"--EPOCH\", default = 2, type=int)\n",
    "parser.add_argument(\"--DEVICE\", default = \"cuda:0\", type=str)\n",
    "parser.add_argument(\"--LR\", default = 0.0001, type=float)\n",
    "args = parser.parse_args(args=[]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.d = \"./數值資料/%s\" % args.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(str_):\n",
    "    path = '%s/%s' % (args.d, str_)\n",
    "    with open(path) as json_file:        \n",
    "        buf = json.load(json_file)\n",
    "        \n",
    "    if( args.t):\n",
    "        buf = buf[:20]\n",
    "        \n",
    "    globals()[str_[:-5]] = buf\n",
    "    print(\"assign %s\" % str_[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assign normal_testing\n"
     ]
    }
   ],
   "source": [
    "if(args.t):\n",
    "    read_data(\"normal_testing.json\")\n",
    "else:\n",
    "    [read_data(i) for i in tqdm(os.listdir(args.d))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buf = [len(i) for i in normal_training]\n",
    "# sns.distplot(buf, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loader(data_, label_, shuffle_):\n",
    "    assert type(data_) == list, \"data is not list\"\n",
    "    assert type(label_) == int, \"label_ is not int\"\n",
    "    assert type(shuffle_) == bool, \"data is not bool\"\n",
    "\n",
    "    buf = [torch.Tensor(i[:args.maxLen]) for i in tqdm(data_) ]\n",
    "    buf = rnn_utils.pad_sequence(buf, batch_first=True)\n",
    "    print(\"共 %s 筆 \\n每筆長度: %s \\n每個時間點有 %s 維\" % (len(buf), len(buf[0]), len(buf[0][0])))\n",
    "\n",
    "    data = torch.FloatTensor(buf) \n",
    "    label = torch.LongTensor([label_]*len(buf)) \n",
    "\n",
    "    data_set = Data.TensorDataset(data, label)\n",
    "    data_loader =  Data.DataLoader(dataset = data_set, \n",
    "                                   batch_size = args.BatchSize, \n",
    "                                   shuffle = shuffle_)\n",
    "    \n",
    "    return(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 11503.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 20 筆 \n",
      "每筆長度: 15 \n",
      "每個時間點有 300 維\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = prepare_data_loader(data_ = normal_testing, \n",
    "                                   label_ = 0, \n",
    "                                   shuffle_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.maxLen = min(len(X[0]), args.maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.maxLen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 測試: 每個大小的都只有兩個\n",
    "        self.conv2 = nn.Conv1d(in_channels = args.embedLen, \n",
    "                               out_channels = 2, \n",
    "                               kernel_size = 2, \n",
    "                               bias = False)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels = args.embedLen, \n",
    "                               out_channels = 2, \n",
    "                               kernel_size = 3, \n",
    "                               bias = False)\n",
    "    \n",
    "        self.conv4 = nn.Conv1d(in_channels = args.embedLen, \n",
    "                               out_channels = 2, \n",
    "                               kernel_size = 4, \n",
    "                               bias = False)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool1d(args.maxLen - 1 , stride = 1)\n",
    "        self.pool3 = nn.MaxPool1d(args.maxLen - 2 , stride = 1)\n",
    "        self.pool4 = nn.MaxPool1d(args.maxLen - 3 , stride = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x_2 = self.conv2(x)\n",
    "        x_3 = self.conv3(x)\n",
    "        x_4 = self.conv4(x)\n",
    "        \n",
    "        p_2 = self.pool2(x_2)\n",
    "        p_3 = self.pool3(x_3)\n",
    "        p_4 = self.pool4(x_4)\n",
    "        \n",
    "        # concat + sequeeze # [batchsize, outputlayer, 1] -> [batchsize, outputlayer]\n",
    "\n",
    "        p2, p3, p4 = p_2.squeeze(), p_3.squeeze(), p_4.squeeze()\n",
    "        \n",
    "        # concat p2 p3 p4 -> [batchsize, 3*outputlayer]\n",
    "        concat = torch.cat(tensors = (p2, p3, p4), dim = 1)\n",
    "        \n",
    "        return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net.to(args.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = net(X.to(args.DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2985, 0.3594, 0.3153, 0.3410, 0.2875, 0.2085],\n",
       "        [0.2724, 0.2409, 0.5157, 0.2613, 0.2653, 0.1507],\n",
       "        [0.3775, 0.3940, 0.1020, 0.1369, 0.3361, 0.0486]], device='cuda:0',\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimize part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr = args.LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        # --------------------------------------------\n",
    "        # Initialization\n",
    "        # --------------------------------------------\n",
    "        super(customLoss, self).__init__()\n",
    "        self.weight = alpha\n",
    "\n",
    "    def forward(self, X):\n",
    "        # --------------------------------------------\n",
    "        # Define forward pass\n",
    "        # --------------------------------------------\n",
    "        # Transform targets to one-hot vector\n",
    "        mean_buf = torch.mean(X, 0) # loss\n",
    "        mean_bs = mean_buf.repeat(len(X), 1)\n",
    "        MinusNorm_norm = torch.norm(X - mean_bs, dim=1)\n",
    "        \n",
    "        two_norm = torch.norm(X, dim=1)\n",
    "        \n",
    "        return torch.mean(MinusNorm_norm + self.weight*two_norm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = customLoss(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(72.2525, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EPOCH 1/2 === 2021-01-21 21:28:48 ===\n",
      "loss: 452.845\n",
      "\n",
      "=== EPOCH 2/2 === 2021-01-21 21:28:48 ===\n",
      "loss: 404.221\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.EPOCH):\n",
    "    print(\"\\n=== EPOCH %d/%d === %s ===\"% (epoch+1, args.EPOCH, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X, y = data\n",
    "        outputs = net(X.to(args.DEVICE))\n",
    "        loss = criterion(outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        #break\n",
    "    #break\n",
    "    print( \"loss: %.3f\" % running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
